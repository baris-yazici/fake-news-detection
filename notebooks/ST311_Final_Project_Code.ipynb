{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"146ZKJIa8K9rm0tMgxhP27KOaZeCHLrkq","timestamp":1714211404545}],"collapsed_sections":["b8mAAsOBux-g","Ak2bb-uJBUnD","WYz5ISA0CRGK","Iog4upOe-UB5","1lZUTg9C-UB6","lu0vNsmXEI6N","Vs_kjEgMExpy","P9Ed3iUfvH42","3Pp_XCoJ2XjM"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### [Colab Link](https://colab.research.google.com/drive/146ZKJIa8K9rm0tMgxhP27KOaZeCHLrkq?usp=drive_link)"],"metadata":{"id":"idx5yrLLI-PA"}},{"cell_type":"markdown","source":["# Fake News Detection using (Small and Large) Language Models\n","\n","- Assignment: ST 311 Final Project\n","- Authors: 24788, 21840"],"metadata":{"id":"z7cd1r0N85if"}},{"cell_type":"markdown","metadata":{"id":"b8mAAsOBux-g"},"source":["# SLMs - LIAR\n","\n","References:\n","- We use the following Hugging Face notebook as a reference: https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/training.ipynb"]},{"cell_type":"code","source":["#!pip install accelerate -U\n","#!pip install transformers datasets\n","#!pip install evaluate"],"metadata":{"id":"WUd-o5mBYbK-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load and Filter the dataset to include only rows where the label is either 0 or 3"],"metadata":{"id":"Ak2bb-uJBUnD"}},{"cell_type":"code","source":["from datasets import DatasetDict\n","from datasets import load_dataset\n","\n","dataset = load_dataset(\"liar\")"],"metadata":{"id":"elEFHimByEVB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define a function to filter and convert labels\n","def filter_and_convert_labels(example):\n","    if example['label'] in [0, 3]:\n","        return {'label': 0 if example['label'] == 0 else 1}\n","    return None"],"metadata":{"id":"mieXabVX-CvD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Apply filtering and label conversion\n","filtered_dataset = DatasetDict({\n","    split: dataset[split].filter(lambda example: example['label'] in [0, 3])\n","    .map(filter_and_convert_labels) for split in dataset.keys()\n","})"],"metadata":{"id":"I2BEquIm-FeA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Display the first 5 entries of the training dataset\n","for i in range(6):\n","    print(filtered_dataset['train'][i])"],"metadata":{"id":"2WhlsMYx6Kcg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Preprocessing LIAR with two labels (TRUE AND FALSE)"],"metadata":{"id":"WYz5ISA0CRGK"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"YcE8Adk7u5d5"},"outputs":[],"source":["from transformers import AutoTokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iofKYyTeu8jk"},"outputs":[],"source":["def preprocess_function(examples):\n","    return tokenizer(examples[\"statement\"], truncation=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BCRNEFtwu_Cb"},"outputs":[],"source":["tokenized_liar_train = filtered_dataset[\"train\"].map(preprocess_function, batched=True)\n","tokenized_liar_test = filtered_dataset[\"test\"].map(preprocess_function, batched=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"01JsWjAXvBdI"},"outputs":[],"source":["from transformers import DataCollatorWithPadding\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rJcLeQrJvHkS"},"outputs":[],"source":["id2label = {0: \"false\", 1: \"true\"}\n","label2id =  {\"false\": 0, \"true\": 1}"]},{"cell_type":"markdown","metadata":{"id":"Iog4upOe-UB5"},"source":["## Helpers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eDdmZmTM-UB5"},"outputs":[],"source":["import evaluate\n","\n","accuracy = evaluate.load(\"accuracy\")\n","\n","import numpy as np\n","\n","def compute_metrics(eval_pred): # will use this inside the training function\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=1)\n","    return accuracy.compute(predictions=predictions, references=labels)\n","\n","def custom_precision_recall_f1(predicted, actual, true_label=1, false_label=0):\n","    true_positives = sum((p == true_label) and (a == true_label) for p, a in zip(predicted, actual))\n","    true_negatives = sum((p == false_label) and (a == false_label) for p, a in zip(predicted, actual))\n","    false_positives = sum((p == true_label) and (a == false_label) for p, a in zip(predicted, actual))\n","    false_negatives = sum((p == false_label) and (a == true_label) for p, a in zip(predicted, actual))\n","\n","    precision_r = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n","    precision_f = true_negatives / (true_negatives + false_negatives) if (true_negatives + false_negatives) > 0 else 0\n","    precision = (precision_r + precision_f) / 2\n","\n","\n","    recall_r = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n","    recall_f = true_negatives / (true_negatives + false_positives) if (true_negatives + false_positives) > 0 else 0\n","    recall = (recall_r + recall_f) / 2\n","\n","    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n","\n","    return precision, recall, f1_score"]},{"cell_type":"code","source":["# Login to the Hugging Face CLI within a Google Colab environment\n","!huggingface-cli login"],"metadata":{"id":"b6NIFKs9X_As"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### For hyperparameters used in training for each SLM, here are the training arguments part of the provided code:\n","\n","- learning_rate: 3e-06\n","- train_batch_size: 8\n","- eval_batch_size: 8\n","- seed: 42\n","- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n","- lr_scheduler_type: linear\n","- num_epochs: 5"],"metadata":{"id":"bvey3hOJntnd"}},{"cell_type":"markdown","metadata":{"id":"1lZUTg9C-UB6"},"source":["## Bert - Training and Evaluation"]},{"cell_type":"code","source":["# Load the pretrained bert-base-cased model\n","from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n","\n","model_binary_bert = AutoModelForSequenceClassification.from_pretrained(\n","      \"bert-base-cased\",\n","      num_labels=2,\n","      id2label=id2label,\n","      label2id=label2id\n",")"],"metadata":{"id":"H-FROnqyLPPB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_args = TrainingArguments(\n","    output_dir=\"liar_binaryclassifier_bert_cased\",\n","    learning_rate=3e-06,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    num_train_epochs=5,\n","    weight_decay=0.1,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    logging_strategy=\"epoch\",\n","    load_best_model_at_end=True,\n","    push_to_hub=True,\n",")\n","\n","trainer = Trainer(\n","    model=model_binary_bert,\n","    args=training_args,\n","    train_dataset=tokenized_liar_train,\n","    eval_dataset=tokenized_liar_test,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")"],"metadata":{"id":"nPU5q2ThN5AJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate pre-trained model's performance\n","pretrained_eval_results = trainer.evaluate()\n","print(\"Pre-trained model performance:\")\n","print(pretrained_eval_results)"],"metadata":{"id":"a_yF-_L2N63g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extract the actual labels\n","actual_results = tokenized_liar_test['label']\n","\n","# Predict labels on the test dataset\n","pretrained_test_results = trainer.predict(tokenized_liar_test)\n","\n","# Extract the predicted labels\n","pretrained_test_predictions = pretrained_test_results.predictions.argmax(axis=1)\n","\n","# Returns the prediction vector from the pre-trained model on the test dataset\n","pretrained_test_predictions"],"metadata":{"id":"L2lxtOxdUxNU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Use pretrained_test_predictions along with actual labels to compute custom precision, recall, and F1 score\n","precision, recall, f1_score = custom_precision_recall_f1(pretrained_test_predictions, actual_results)\n","print(\"precision:\", precision, \"recall:\", recall, \"f1_score:\", f1_score)"],"metadata":{"id":"huwm_VGGVI6e"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-lQqcS5kxvoF"},"outputs":[],"source":["# Fine-tune all layers of the pre-trained model\n","trainer.train() # call this to start training"]},{"cell_type":"code","source":["# Push the trained model to the Hugging Face model hub\n","trainer.push_to_hub()"],"metadata":{"id":"ZBEfavfFX3dv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extract the actual labels\n","actual = tokenized_liar_test['label']\n","\n","# Evaluate trained model results\n","eval_results = trainer.evaluate()\n","print(\"Fine-tuned model performance:\")\n","print(eval_results)\n","\n","# Predict labels on the test dataset\n","test_results = trainer.predict(tokenized_liar_test)\n","\n","# Extract the predicted labels\n","test_predictions = test_results.predictions.argmax(axis=1)\n","\n","# Returns the prediction vector from the fine-tuned model on the test dataset\n","pretrained_test_predictions"],"metadata":{"id":"OHdnR71hDC4J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Use test_predictions along with actual labels to compute custom precision, recall, and F1 score\n","precision, recall, f1_score = custom_precision_recall_f1(test_predictions, actual_results)\n","\n","print(\"precision:\", precision, \"recall:\", recall, \"f1_score:\", f1_score)"],"metadata":{"id":"Nild_2ozAAn7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lu0vNsmXEI6N"},"source":["## DistilBert - Training and Evaluation"]},{"cell_type":"code","source":["# Load the pretrained distilbert-base-cased model\n","from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n","\n","model_binary_distilbert = AutoModelForSequenceClassification.from_pretrained(\n","      \"bert-base-cased\",\n","      num_labels=2,\n","      id2label=id2label,\n","      label2id=label2id\n",")"],"metadata":{"id":"FQCFmUicLVUR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_args = TrainingArguments(\n","    output_dir=\"liar_binaryclassifier_distilbert_cased\",\n","    learning_rate=3e-06,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    num_train_epochs=5,\n","    weight_decay=0.1,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    logging_strategy=\"epoch\",\n","    load_best_model_at_end=True,\n","    push_to_hub=True,\n",")\n","\n","trainer = Trainer(\n","    model=model_binary_distilbert,\n","    args=training_args,\n","    train_dataset=tokenized_liar_train,\n","    eval_dataset=tokenized_liar_test,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")"],"metadata":{"id":"0X5bD2YEEI6O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate pre-trained model's performance\n","pretrained_eval_results = trainer.evaluate()\n","print(\"Pre-trained model performance:\")\n","print(pretrained_eval_results)"],"metadata":{"id":"UY7mM3aCEI6P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extract the actual labels\n","actual_results = tokenized_liar_test['label']\n","\n","# Predict labels on the test dataset\n","pretrained_test_results = trainer.predict(tokenized_liar_test)\n","\n","# Extract the predicted labels\n","pretrained_test_predictions = pretrained_test_results.predictions.argmax(axis=1)\n","\n","# Returns the prediction vector from the pre-trained model on the test dataset\n","pretrained_test_predictions"],"metadata":{"id":"qUMSJJDVEI6Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Use pretrained_test_predictions along with actual labels to compute custom precision, recall, and F1 score\n","precision, recall, f1_score = custom_precision_recall_f1(pretrained_test_predictions, actual_results)\n","print(\"precision:\", precision, \"recall:\", recall, \"f1_score:\", f1_score)"],"metadata":{"id":"ZNPr9uOVEI6R"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J25RvfCBEI6S"},"outputs":[],"source":["# Fine-tune all layers of the pre-trained model\n","trainer.train() # call this to start training"]},{"cell_type":"code","source":["# Push the trained model to the Hugging Face model hub\n","trainer.push_to_hub()"],"metadata":{"id":"Nq4qI1ZSEI6T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extract the actual labels\n","actual = tokenized_liar_test['label']\n","\n","# Evaluate trained model results\n","eval_results = trainer.evaluate()\n","print(\"Fine-tuned model performance:\")\n","print(eval_results)\n","\n","# Predict labels on the test dataset\n","test_results = trainer.predict(tokenized_liar_test)\n","\n","# Extract the predicted labels\n","test_predictions = test_results.predictions.argmax(axis=1)\n","\n","# Returns the prediction vector from the fine-tuned model on the test dataset\n","pretrained_test_predictions"],"metadata":{"id":"QxAOfYvAEI6Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Use test_predictions along with actual labels to compute custom precision, recall, and F1 score\n","precision, recall, f1_score = custom_precision_recall_f1(test_predictions, actual_results)\n","\n","print(\"precision:\", precision, \"recall:\", recall, \"f1_score:\", f1_score)"],"metadata":{"id":"n2U6XrZNEI6a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vs_kjEgMExpy"},"source":["## Roberta - Training and Evaluation"]},{"cell_type":"code","source":["# Load the pretrained roberta-base model\n","from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n","\n","model_binary_roberta = AutoModelForSequenceClassification.from_pretrained(\n","      \"roberta-base\",\n","      num_labels=2,\n","      id2label=id2label,\n","      label2id=label2id\n",")"],"metadata":{"id":"JGQLowhjLso4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_args = TrainingArguments(\n","    output_dir=\"liar_binaryclassifier_roberta_base\",\n","    learning_rate=3e-06,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    num_train_epochs=5,\n","    weight_decay=0.1,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    logging_strategy=\"epoch\",\n","    load_best_model_at_end=True,\n","    push_to_hub=True,\n",")\n","\n","trainer = Trainer(\n","    model=model_binary_roberta,\n","    args=training_args,\n","    train_dataset=tokenized_liar_train,\n","    eval_dataset=tokenized_liar_test,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")"],"metadata":{"id":"1dee6KyaPN40"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate pre-trained model's performance\n","pretrained_eval_results = trainer.evaluate()\n","print(\"Pre-trained model performance:\")\n","print(pretrained_eval_results)"],"metadata":{"id":"63YVzH6iExpz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extract the actual labels\n","actual_results = tokenized_liar_test['label']\n","\n","# Predict labels on the test dataset\n","pretrained_test_results = trainer.predict(tokenized_liar_test)\n","\n","# Extract the predicted labels\n","pretrained_test_predictions = pretrained_test_results.predictions.argmax(axis=1)\n","\n","# Returns the prediction vector from the pre-trained model on the test dataset\n","pretrained_test_predictions"],"metadata":{"id":"X2mU1UCuExp0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Use pretrained_test_predictions along with actual labels to compute custom precision, recall, and F1 score\n","precision, recall, f1_score = custom_precision_recall_f1(pretrained_test_predictions, actual_results)\n","print(\"precision:\", precision, \"recall:\", recall, \"f1_score:\", f1_score)"],"metadata":{"id":"OoJsxHTmExp1"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CyRG-NLkExp1"},"outputs":[],"source":["# Fine-tune all layers of the pre-trained model\n","trainer.train() # call this to start training"]},{"cell_type":"code","source":["# Push the trained model to the Hugging Face model hub\n","trainer.push_to_hub()"],"metadata":{"id":"1RzwdwTQExqt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extract the actual labels\n","actual = tokenized_liar_test['label']\n","\n","# Evaluate trained model results\n","eval_results = trainer.evaluate()\n","print(\"Fine-tuned model performance:\")\n","print(eval_results)\n","\n","# Predict labels on the test dataset\n","test_results = trainer.predict(tokenized_liar_test)\n","\n","# Extract the predicted labels\n","test_predictions = test_results.predictions.argmax(axis=1)\n","\n","# Returns the prediction vector from the fine-tuned model on the test dataset\n","pretrained_test_predictions"],"metadata":{"id":"F0XYo1E0Exrc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Use test_predictions along with actual labels to compute custom precision, recall, and F1 score\n","precision, recall, f1_score = custom_precision_recall_f1(test_predictions, actual_results)\n","\n","print(\"precision:\", precision, \"recall:\", recall, \"f1_score:\", f1_score)"],"metadata":{"id":"Ffsuk8E4Exrd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# LLMs - LIAR\n","\n","References:\n","\n","- For prompt tuning, we use the following Hugging Face guide as a reference: https://huggingface.co/docs/peft/task_guides/prompt_based_methods?configurations=prompt+tuning\n","\n","- For Unsloth + SFT tuning, we use this Unsloth guide as a reference: https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp?usp=sharing\n"],"metadata":{"id":"GtIDx0twJ0Hr"}},{"cell_type":"markdown","source":["##  Training (Prompt Tuning) and Evaluation\n","\n","### Used with: stablelm-2-zephyr-1_6b, bloomz-1b1, bloomz-560m"],"metadata":{"id":"P9Ed3iUfvH42"}},{"cell_type":"code","source":["# !pip install -q peft transformers datasets"],"metadata":{"id":"lErjyoZpUwgM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM\n","from peft import get_peft_config, get_peft_model, PromptTuningInit, PromptTuningConfig, TaskType, PeftType\n","import torch\n","from datasets import load_dataset\n","import os\n","from transformers import AutoTokenizer\n","from torch.utils.data import DataLoader\n","from transformers import default_data_collator, get_linear_schedule_with_warmup\n","from tqdm import tqdm"],"metadata":{"id":"mcTnH2gxA1bf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Below we set the model as \"stabilityai/stablelm-2-zephyr-1_6b\", but the rest of the notebook is applicable to bloomz-1b1 and bloomz-560m as well."],"metadata":{"id":"kdRq9nlk0iz5"}},{"cell_type":"code","source":["# Choose the model, can use any LLM suitable for text generation\n","model_name_or_path = \"stabilityai/stablelm-2-zephyr-1_6b\"\n","tokenizer_name_or_path = \"stabilityai/stablelm-2-zephyr-1_6b\""],"metadata":{"id":"_rNLSsyR0YGg"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"71fbfca2"},"outputs":[],"source":["device = \"cuda\"\n","\n","peft_config = PromptTuningConfig( # This creates the PEFT configuration, used in loading the model later\n","    task_type=TaskType.CAUSAL_LM, # Text generation\n","    prompt_tuning_init=PromptTuningInit.TEXT, # Initiate the prompt for prompt tuning\n","    num_virtual_tokens=8,\n","    prompt_tuning_init_text=\"Predict if the statement is true or false.\", # This is the prompt initiated for prompt tuning\n","    tokenizer_name_or_path=model_name_or_path,\n",")\n","\n","dataset_name = \"liar\"\n","checkpoint_name = f\"{dataset_name}_{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}_v1.pt\".replace(\n","    \"/\", \"_\"\n",")\n","text_column = \"statement\"\n","label_column = \"label\""]},{"cell_type":"code","source":["# hyperparameters\n","max_length = 64 # max length generated for text, if higher inference takes longer\n","lr = 3e-3 # learning rate, low is better for LLM fine tuning\n","num_epochs = 2\n","batch_size = 8 # higher requires higher memory, 8 is used often"],"metadata":{"id":"NZZvPdn7A8T0"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e1a3648b"},"outputs":[],"source":["# load and filter dataset: liar\n","from datasets import load_dataset\n","\n","dataset = load_dataset(dataset_name)\n","\n","filtered_dataset = dataset.filter(lambda example: example[\"label\"] in [0, 3])\n","\n","classes = [k.replace(\"_\", \" \") for k in filtered_dataset[\"train\"].features[\"label\"].names]\n","\n","filtered_dataset = filtered_dataset.map(\n","    lambda x: {\"label\": [classes[label] for label in x[\"label\"]]},\n","    batched=True,\n","    num_proc=1,\n",")\n","filtered_dataset[\"train\"][0] # print a row to check"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fe12d4d3"},"outputs":[],"source":["# data preprocessing\n","tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n","if tokenizer.pad_token_id is None:\n","    tokenizer.pad_token_id = tokenizer.eos_token_id\n","target_max_length = max([len(tokenizer(class_label)[\"input_ids\"]) for class_label in classes])\n","\n","'''\n","Create a preprocessing function that tokenizes the tweet text and labels,\n","  pad the inputs and labels in each batch,\n","  create an attention mask,\n","  and truncate sequences to the max_length.\n","  Then convert the input_ids, attention_mask, and labels to PyTorch tensors.\n","'''\n","def preprocess_function(examples):\n","    batch_size = len(examples[text_column])\n","    inputs = [f\"{text_column} : {x} Label : \" for x in examples[text_column]]\n","    targets = [str(x) for x in examples[label_column]]\n","    model_inputs = tokenizer(inputs)\n","    labels = tokenizer(targets, add_special_tokens=False)  # don't add bos token because we concatenate with inputs\n","    for i in range(batch_size):\n","        sample_input_ids = model_inputs[\"input_ids\"][i]\n","        label_input_ids = labels[\"input_ids\"][i] + [tokenizer.eos_token_id]\n","        model_inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids\n","        labels[\"input_ids\"][i] = [-100] * len(sample_input_ids) + label_input_ids\n","        model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\n","    for i in range(batch_size):\n","        sample_input_ids = model_inputs[\"input_ids\"][i]\n","        label_input_ids = labels[\"input_ids\"][i]\n","        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n","            max_length - len(sample_input_ids)\n","        ) + sample_input_ids\n","        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n","            \"attention_mask\"\n","        ][i]\n","        labels[\"input_ids\"][i] = [-100] * (max_length - len(sample_input_ids)) + label_input_ids\n","        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n","        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n","        labels[\"input_ids\"][i] = torch.tensor(labels[\"input_ids\"][i][:max_length])\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs\n","\n","# process the train dataset using train_preprocess_function\n","processed_datasets = filtered_dataset.map(\n","    preprocess_function,\n","    batched=True,\n","    num_proc=1,\n","    remove_columns=filtered_dataset[\"train\"].column_names, # remove not needed columns\n","    load_from_cache_file=False,\n","    desc=\"Running tokenizer on dataset\",\n",")\n","\n","train_dataset = processed_datasets[\"train\"]\n","eval_dataset = processed_datasets[\"train\"]\n","\n","# create a data loader to be used in training\n","train_dataloader = DataLoader(\n","    train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True\n",")\n","eval_dataloader = DataLoader(eval_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"641b21fe"},"outputs":[],"source":["def test_preprocess_function(examples):\n","    batch_size = len(examples[text_column])\n","    inputs = [f\"{text_column} : {x} Label : \" for x in examples[text_column]]\n","    model_inputs = tokenizer(inputs)\n","    for i in range(batch_size):\n","        sample_input_ids = model_inputs[\"input_ids\"][i]\n","        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n","            max_length - len(sample_input_ids)\n","        ) + sample_input_ids\n","        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n","            \"attention_mask\"\n","        ][i]\n","        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n","        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n","    return model_inputs\n","\n","# process the test dataset using test_preprocess_function\n","test_dataset = filtered_dataset[\"test\"].map(\n","    test_preprocess_function,\n","    batched=True,\n","    num_proc=1,\n","    remove_columns=filtered_dataset[\"train\"].column_names,\n","    load_from_cache_file=False,\n","    desc=\"Running tokenizer on dataset\",\n",")\n","\n","test_dataloader = DataLoader(test_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a773e092"},"outputs":[],"source":["# creating the model\n","model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n","model = get_peft_model(model, peft_config)\n","model.print_trainable_parameters() # prints the number of parameters we can train, very low for PEFT!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b2f91568"},"outputs":[],"source":["# optimizer and lr scheduler\n","optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n","lr_scheduler = get_linear_schedule_with_warmup(\n","    optimizer=optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=(len(train_dataloader) * num_epochs),\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e4fb69fc"},"outputs":[],"source":["# training and evaluation\n","model = model.to(device)\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    for step, batch in enumerate(tqdm(train_dataloader)):\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        outputs = model(**batch)\n","        loss = outputs.loss\n","        total_loss += loss.detach().float()\n","        loss.backward()\n","        optimizer.step()\n","        lr_scheduler.step()\n","        optimizer.zero_grad()\n","\n","    model.eval()\n","    eval_loss = 0\n","    eval_preds = []\n","    for step, batch in enumerate(tqdm(eval_dataloader)):\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        with torch.no_grad():\n","            outputs = model(**batch)\n","        loss = outputs.loss\n","        eval_loss += loss.detach().float()\n","        eval_preds.extend(\n","            tokenizer.batch_decode(torch.argmax(outputs.logits, -1).detach().cpu().numpy(), skip_special_tokens=True)\n","        )\n","\n","    eval_epoch_loss = eval_loss / len(eval_dataloader)\n","    eval_ppl = torch.exp(eval_epoch_loss)\n","    train_epoch_loss = total_loss / len(train_dataloader)\n","    train_ppl = torch.exp(train_epoch_loss)\n","    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d8ba1f8c"},"outputs":[],"source":["# saving the model\n","peft_model_id = f\"{dataset}_{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}\".replace(\n","    \"/\", \"_\"\n",")\n","model.save_pretrained(peft_model_id)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4d9476e1"},"outputs":[],"source":["# loading the fine-tuned model\n","from peft import PeftModel, PeftConfig\n","\n","peft_model_id = f\"{dataset_new}_{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}\".replace(\n","    \"/\", \"_\"\n",")\n","\n","# we need to specify PEFT configuration every time we want to load a model fine-tuned with PEFT\n","config = PeftConfig.from_pretrained(peft_model_id)\n","model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\n","model = PeftModel.from_pretrained(model, peft_model_id)"]},{"cell_type":"code","source":["# inference, calculate test predictions\n","import re\n","\n","model.to(device)\n","model.eval()\n","\n","predicted_labels = []\n","\n","for i in range(len(filtered_dataset[\"test\"])):\n","    inputs = tokenizer(f'{text_column} : {filtered_dataset[\"test\"][i][\"statement\"]} Label : ', return_tensors=\"pt\")\n","\n","    with torch.no_grad():\n","        inputs = {k: v.to(device) for k, v in inputs.items()}\n","        outputs = model.generate(\n","            input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_new_tokens=10, eos_token_id=3\n","        )\n","        generated_text = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)\n","\n","        # Extracting the label from the generated text using regular expressions\n","        label_text = generated_text[0]\n","        label_match = re.search(r'Label : (\\d+)', label_text)\n","        if label_match:\n","            label = int(label_match.group(1))\n","            predicted_labels.append(label)\n","        else:\n","            print(f\"Label not found in generated text: {label_text}\")\n","\n","print(predicted_labels)\n","len(predicted_labels) # check the length of labels vector to see if it matches the test set size"],"metadata":{"id":"PcmffLM8jbS3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# extract the real labels from the test set\n","actual_labels = []\n","\n","for i in range(len(filtered_dataset[\"test\"])):\n","    actual_label = filtered_dataset[\"test\"][i][\"label\"]\n","    actual_labels.append(actual_label)\n","\n","print(actual_labels)"],"metadata":{"id":"bvym78STlIRG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# calculate accuracy\n","correct_predictions = sum(1 for pred, actual in zip(predicted_labels, actual_labels) if pred == actual)\n","total_predictions = len(predicted_labels)\n","accuracy = correct_predictions / total_predictions\n","\n","print(f\"Accuracy: {accuracy:.2%}\")"],"metadata":{"id":"NLPMMvFllYJv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# function to calculate precision, recall, and f1\n","def custom_precision_recall_f1(predicted, actual, true_label=3, false_label=0):\n","    true_positives = sum((p == true_label) and (a == true_label) for p, a in zip(predicted, actual))\n","    true_negatives = sum((p == false_label) and (a == false_label) for p, a in zip(predicted, actual))\n","    false_positives = sum((p == true_label) and (a == false_label) for p, a in zip(predicted, actual))\n","    false_negatives = sum((p == false_label) and (a == true_label) for p, a in zip(predicted, actual))\n","\n","    precision_r = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n","    precision_f = true_negatives / (true_negatives + false_negatives) if (true_negatives + false_negatives) > 0 else 0\n","    precision = (precision_r + precision_f) / 2\n","\n","\n","    recall_r = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n","    recall_f = true_negatives / (true_negatives + false_positives) if (true_negatives + false_positives) > 0 else 0\n","    recall = (recall_r + recall_f) / 2\n","\n","    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n","\n","    return precision, recall, f1_score"],"metadata":{"id":"EDFvIKC3Krkb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print precision, recall, and f1\n","custom_precision_recall_f1(predicted_labels, actual_labels)"],"metadata":{"id":"PiLNIomsKsyR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##  Training (Unsloth + SFT) and Evaluation\n","\n","### Used with: llama-3-8b-bnb-4bit, gemma-7b-bnb-4bit, mistral-7b-bnb-4bit, tinyllama-bnb-4bit"],"metadata":{"id":"3Pp_XCoJ2XjM"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"M8xdj2W3NoaY"},"outputs":[],"source":["#! pip install git+https://github.com/huggingface/transformers.git\n","#! pip install 'transformers>=3.9.1' accelerate\n","#! pip install -i https://pypi.org/simple/ bitsandbytes\n","#! pip install datasets"]},{"cell_type":"markdown","source":["Installing Unsloth as suggested by Hugging Face:"],"metadata":{"id":"i3gPQQZV3bp3"}},{"cell_type":"code","source":["%%capture\n","import torch\n","major_version, minor_version = torch.cuda.get_device_capability()\n","# Must install separately since Colab has torch 2.2.1, which breaks packages\n","!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n","if major_version >= 8:\n","    # Use this for new GPUs like Ampere, Hopper GPUs (RTX 30xx, RTX 40xx, A100, H100, L40)\n","    !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n","else:\n","    # Use this for older GPUs (V100, Tesla T4, RTX 20xx)\n","    !pip install --no-deps xformers trl peft accelerate bitsandbytes\n","pass"],"metadata":{"id":"8HiOymCejWfG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Import the models using the Fast Language Model framework from Unsloth."],"metadata":{"id":"ydOlRnW73h3U"}},{"cell_type":"code","source":["from unsloth import FastLanguageModel\n","import torch\n","max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n","dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n","load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n","\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = \"unsloth/gemma-7b-bnb-4bit\", # we use gemma-7b-bnb-4bit but rest of the notebook is fully applicable to other models mentioned\n","    max_seq_length = max_seq_length,\n","    dtype = dtype,\n","    load_in_4bit = load_in_4bit,\n","    token = \"put_your_own_HF_token_here\", # needed for gated models like meta-llama/Llama-3-8b\n",")"],"metadata":{"id":"Sd0TRSE3jU4h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We give access to our drive to save predictions to a drive folder as they are generated so that in case of interrupted connection we don't need to re-do all the inference."],"metadata":{"id":"3lkOCEE-3zcA"}},{"cell_type":"code","source":["import pandas as pd\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"t5kvXf2gridn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A PEFT configuration is needed for fast language models as well. Note that this is different from the prompt tuning we used in the previous section and needed to use the customized Unsloth models."],"metadata":{"id":"xfrc0n454AT8"}},{"cell_type":"code","source":["model = FastLanguageModel.get_peft_model(\n","    model,\n","    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n","    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n","    lora_alpha = 16,\n","    lora_dropout = 0, # Supports any, but = 0 is optimized\n","    bias = \"none\",    # Supports any, but = \"none\" is optimized\n","    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n","    random_state = 3407,\n","    use_rslora = False,\n","    loftq_config = None, # And LoftQ\n",")"],"metadata":{"id":"ZJztCJtbt_On"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load and preprocess the Liar dataset. A version with instruction and output columns needed for supervised fine tuning."],"metadata":{"id":"s581w-ek4Rs-"}},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","# Define the function to add columns\n","def add_columns(example):\n","    instruction = \"Analyze the following statement and decide if it is true or false.\"\n","    output = \"The statement is true.\" if example[\"label\"] == 3 else \"The statement is false.\"\n","    example[\"instruction\"] = instruction\n","    example[\"output\"] = output\n","    return example\n","\n","# Load the dataset\n","dataset = load_dataset(\"liar\")\n","\n","# Filter the dataset\n","filtered_dataset_train = dataset[\"train\"].filter(lambda example: example[\"label\"] in [0, 3])\n","filtered_dataset_test = dataset[\"test\"].filter(lambda example: example[\"label\"] in [0, 3])\n","\n","# Add new columns \"instruction\" and \"output\" to each example\n","filtered_dataset_train = filtered_dataset_train.map(add_columns)\n","\n","# Print a sample example to verify the addition of the new columns\n","print(filtered_dataset_train[0])"],"metadata":{"id":"X278lsGx2Cx3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We create an alpaca prompt to use in training and inference and preprocess the dataset to have a single text column with instruction, input, and response."],"metadata":{"id":"vSmRkWw94eCB"}},{"cell_type":"code","source":["alpaca_prompt =\n","\"\"\"\n","Below is an instruction that describes a task,\n","paired with an input that provides further context.\n","Write a response that appropriately completes the request.\n","\n","### Instruction:\n","{}\n","\n","### Input:\n","{}\n","\n","### Response:\n","{}\n","\"\"\"\n","\n","EOS_TOKEN = tokenizer.eos_token # Get the EOS_TOKEN of the tokenizer\n","def formatting_prompts_func(examples):\n","    instructions = examples[\"instruction\"]\n","    inputs       = examples[\"statement\"]\n","    outputs      = examples[\"output\"]\n","    texts = []\n","    for instruction, input, output in zip(instructions, inputs, outputs):\n","        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n","        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n","        texts.append(text)\n","    return { \"text\" : texts, }\n","pass\n","\n","from datasets import load_dataset\n","dataset = filtered_dataset_train.map(formatting_prompts_func, batched = True,)"],"metadata":{"id":"2asOBsMq0KIM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, load the SFTTrainer from Hugging Face and set training arguments."],"metadata":{"id":"1hM_fGpO45e0"}},{"cell_type":"code","source":["from trl import SFTTrainer\n","from transformers import TrainingArguments\n","\n","trainer = SFTTrainer(\n","    model = model,\n","    tokenizer = tokenizer,\n","    train_dataset = dataset,\n","    dataset_text_field = \"text\",\n","    max_seq_length = max_seq_length,\n","    dataset_num_proc = 2,\n","    packing = False, # Can make training 5x faster for short sequences.\n","    args = TrainingArguments(\n","        per_device_train_batch_size = 2,\n","        gradient_accumulation_steps = 4,\n","        warmup_steps = 5,\n","        num_train_epochs = 1,\n","        #max_steps = 120, # could be used if we don't want full epoch runs\n","        learning_rate = 2e-4,\n","        fp16 = not torch.cuda.is_bf16_supported(),\n","        bf16 = torch.cuda.is_bf16_supported(),\n","        logging_steps = 1,\n","        optim = \"adamw_8bit\",\n","        weight_decay = 0.01,\n","        lr_scheduler_type = \"linear\",\n","        seed = 3407,\n","        output_dir = \"outputs\",\n","    ),\n",")"],"metadata":{"id":"aAsV8mDw2sZ7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Run the training."],"metadata":{"id":"ly0MvoYx5Irt"}},{"cell_type":"code","source":["trainer_stats = trainer.train()"],"metadata":{"id":"Jk2G3IzB25xG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we do inference and save the predictions.\n","\n","Note that for base models, we start from here and just do predictions."],"metadata":{"id":"SkXPcPhK5MgO"}},{"cell_type":"code","source":["import json\n","\n","# Initialize an empty list to store the results\n","results = []\n","\n","# Specify the file path where you want to save the results\n","file_path = \"/content/drive/My Drive/results_liar_gemma7b_finetuned.json\"  # Change the path as needed\n","\n","# Define a function to save results to a file\n","def save_results(results, file_path):\n","    with open(file_path, 'w') as f:\n","        json.dump(results, f)\n","\n","# Define the Alpaca prompt\n","alpaca_prompt =\n","\"\"\"Below is an instruction that describes a task,\n","paired with an input that provides further context.\n","Write a response that appropriately completes the request.\n","\n","### Instruction:\n","{}\n","\n","### Input:\n","{}\n","\n","### Response:\n","{}\"\"\"\n","FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","\n","for example in filtered_dataset_test:\n","    inputs = tokenizer(\n","    [\n","        alpaca_prompt.format(\n","            \"Analyze the news provided and decide if it is true or false.\", # instruction\n","            f\"{example['statement']}\", # input\n","            \"\", # output - leave this blank for generation!\n","        )\n","    ], return_tensors = \"pt\").to(\"cuda\")\n","\n","    outputs = model.generate(**inputs, max_new_tokens = 128, use_cache = True)\n","    generated_text = tokenizer.batch_decode(outputs)\n","\n","    print(generated_text)\n","    # Append the results to the list\n","    results.append({'statement': example['statement'], 'generated_text': generated_text, 'label': example['label']})\n","\n","    # Periodically save the results to the file\n","    if len(results) % 10 == 0:  # Save every 10 predictions\n","        save_results(results, file_path)\n","        print(\"Result saved\")\n","\n","# Save the final results to the file\n","save_results(results, file_path)"],"metadata":{"id":"e7C1diITWyw2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Accuracy calculation."],"metadata":{"id":"pBBBkuuP5fVK"}},{"cell_type":"code","source":["import json\n","\n","# Specify the file path from where you want to load the results\n","file_path = \"/content/drive/My Drive/results_liar_gemma7b_finetuned.json\"  # Change the path as needed\n","\n","# load a json file\n","def load_results(file_path):\n","    with open(file_path, 'r') as f:\n","        results = json.load(f)\n","    return results\n","\n","loaded_results = load_results(file_path)"],"metadata":{"id":"6kwpUyB30Wq-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# function to calculate accuracy, save extract predicted and actual labels\n","def calculate_accuracy(results):\n","    correct_predictions = 0\n","    total_predictions = len(results)\n","    prediction_labels = []\n","    actual_labels = []\n","\n","    # this for loop is necessary because the answers generated are not always in the same format\n","    for entry in results:\n","        # extract the generated_text\n","        generated_text = entry['generated_text'][0]\n","\n","        # find the index where the response starts\n","        start_index = generated_text.find(\"\\n\\n### Response:\\n\")\n","\n","        # extract the response of the generated text\n","        response_part = generated_text[start_index + len(\"\\n\\n### Response:\\n\"):]\n","\n","        generated_decision = None\n","\n","        # split the response by line\n","        response_lines = response_part.split('\\n')\n","        print(response_lines)\n","\n","        true_line_number = None\n","        false_line_number = None\n","\n","        # loop through each line in the response to find the indices of \"true\" and \"false\"\n","        for line_number, line in enumerate(response_lines):\n","            line = line.strip()  # Remove whitespace\n","            if \"true\" in line.lower():\n","                true_line_number = line_number\n","                if false_line_number is not None:  # If both \"true\" and \"false\" are found, break the loop\n","                    break\n","            elif \"false\" in line.lower():\n","                false_line_number = line_number\n","                if true_line_number is not None:  # If both \"true\" and \"false\" are found, break the loop\n","                    break\n","\n","        # determine the decision based on which index first\n","        if true_line_number is not None and (false_line_number is None or true_line_number < false_line_number):\n","            generated_decision = \"true\"\n","        elif false_line_number is not None and (true_line_number is None or false_line_number < true_line_number):\n","            generated_decision = \"false\"\n","        else:\n","            generated_decision = None  # Neither \"true\" nor \"false\" found or found at the same line number\n","\n","        # if no True or False found, default to \"false\" because we cannot be sure\n","        if generated_decision is None:\n","            generated_decision = \"false\"\n","\n","        # convert the generated decision to label (0 for False and 3 for True)\n","        generated_label = 3 if generated_decision == 'true' else 0\n","\n","        # get the actual label\n","        actual_label = entry['label']\n","\n","        # append the label to use later\n","        prediction_labels.append(generated_label)\n","        actual_labels.append(actual_label)\n","\n","        # check if the generated label matches the actual label\n","        if generated_label == actual_label:\n","            correct_predictions += 1\n","\n","    # calculate accuracy\n","    accuracy = correct_predictions / total_predictions * 100\n","    return accuracy, prediction_labels, actual_labels\n","\n","# output\n","accuracy, prediction_labels, actual_labels = calculate_accuracy(loaded_results)\n","print(\"Accuracy:\", accuracy, \"%\")"],"metadata":{"id":"nBC9JykatuB6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# function to calculate precision, recall, and f1 using prediction and actual labels\n","def custom_precision_recall_f1(predicted, actual, true_label=3, false_label=0):\n","    true_positives = sum((p == true_label) and (a == true_label) for p, a in zip(predicted, actual))\n","    true_negatives = sum((p == false_label) and (a == false_label) for p, a in zip(predicted, actual))\n","    false_positives = sum((p == true_label) and (a == false_label) for p, a in zip(predicted, actual))\n","    false_negatives = sum((p == false_label) and (a == true_label) for p, a in zip(predicted, actual))\n","\n","    precision_r = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n","    precision_f = true_negatives / (true_negatives + false_negatives) if (true_negatives + false_negatives) > 0 else 0\n","    precision = (precision_r + precision_f) / 2\n","\n","\n","    recall_r = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n","    recall_f = true_negatives / (true_negatives + false_positives) if (true_negatives + false_positives) > 0 else 0\n","    recall = (recall_r + recall_f) / 2\n","\n","    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n","\n","    return precision, recall, f1_score"],"metadata":{"id":"sDZpW4SS3O7r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print precision, recall, and f1\n","custom_precision_recall_f1(prediction_labels, actual_labels)"],"metadata":{"id":"QtpnavHjP87z"},"execution_count":null,"outputs":[]}]}